{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "81246087-f81b-423d-8c8a-f6c7014e856c",
      "metadata": {
        "id": "81246087-f81b-423d-8c8a-f6c7014e856c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
        "\n",
        "## First, check to see if lightning is installed, if not, install it.\n",
        "import pip\n",
        "try:\n",
        "  __import__(\"lightning\")\n",
        "except ImportError:\n",
        "  pip.main(['install', \"lightning\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5c520e0b-c6e4-43ce-93f5-c0f2b5e75438",
      "metadata": {
        "id": "5c520e0b-c6e4-43ce-93f5-c0f2b5e75438"
      },
      "outputs": [],
      "source": [
        "import torch ## torch let's us create tensors and also provides helper functions\n",
        "import torch.nn as nn ## torch.nn gives us nn.Module, nn.Embedding() and nn.Linear()\n",
        "import torch.nn.functional as F # This gives us the softmax() and argmax()\n",
        "from torch.optim import Adam # This is the optimizer we will use\n",
        "\n",
        "import lightning as L # Lightning makes it easier to write, optimize and scale our code\n",
        "from torch.utils.data import TensorDataset, DataLoader # We'll store our data in DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e58ebb2-1798-41c3-9ff8-1b4f50605964",
      "metadata": {
        "id": "6e58ebb2-1798-41c3-9ff8-1b4f50605964"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa9ddca3-53b6-451b-8fe1-6116e1f7f473",
      "metadata": {
        "id": "fa9ddca3-53b6-451b-8fe1-6116e1f7f473"
      },
      "source": [
        "# The input and output vocabularies and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "63631347-8db6-4812-8198-9169b2df0b24",
      "metadata": {
        "id": "63631347-8db6-4812-8198-9169b2df0b24"
      },
      "outputs": [],
      "source": [
        "## first, a dictionary for the input vocabulary\n",
        "input_vocab = {'<SOS>': 0, ## <SOS> = start of sequence.\n",
        "               'lets': 1,\n",
        "               'to': 2,\n",
        "               'go': 3}\n",
        "\n",
        "## Now a dictionary for the output vocabulary\n",
        "output_vocab = {'<SOS>': 0,\n",
        "                'ir': 1,\n",
        "                'vamos': 2,\n",
        "                'y': 3,\n",
        "                '<EOS>': 4}\n",
        "\n",
        "\n",
        "inputs = torch.tensor([[1, 3],\n",
        "                       [2, 3]])\n",
        "\n",
        "## Here are the spanish translations encoded using the output vocabulary\n",
        "labels = torch.tensor([[2],\n",
        "                      [1]])\n",
        "\n",
        "dataset = TensorDataset(inputs, labels)\n",
        "dataloader = DataLoader(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3de72068-c081-4868-9e60-1f5cc6cf45df",
      "metadata": {
        "id": "3de72068-c081-4868-9e60-1f5cc6cf45df"
      },
      "source": [
        "Now that we have created the input and output datasets and the **Dataloader** to train the model, let's start building it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42efbdf7-b3de-4b12-b2ec-a54651b9df79",
      "metadata": {
        "id": "42efbdf7-b3de-4b12-b2ec-a54651b9df79"
      },
      "source": [
        "<a id=\"position\"></a>\n",
        "# Position Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b8b62789-ee84-49bf-b736-12c1b47b34ae",
      "metadata": {
        "id": "b8b62789-ee84-49bf-b736-12c1b47b34ae"
      },
      "outputs": [],
      "source": [
        "class PositionEncoding(nn.Module):\n",
        "    def __init__(self, d_model=2, max_len=3):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
        "\n",
        "\n",
        "        div_term = 1/torch.tensor(10000.0)**(torch.arange(start=0, end=d_model, step=2).float() / d_model)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) \n",
        "        pe[:, 1::2] = torch.cos(position * div_term) \n",
        "\n",
        "        self.register_buffer('pe', pe) \n",
        "\n",
        "    def forward(self, x):\n",
        " \n",
        "        return x + self.pe[:x.size(0), :] "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c20eccdc-3188-4c6d-94da-d28db057dc24",
      "metadata": {
        "id": "c20eccdc-3188-4c6d-94da-d28db057dc24"
      },
      "source": [
        "<a id=\"attention\"></a>\n",
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e3392130-cd25-4000-97bb-9612764c83a8",
      "metadata": {
        "id": "e3392130-cd25-4000-97bb-9612764c83a8"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module): \n",
        "    def __init__(self, d_model=2):\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "        self.row_dim = 0\n",
        "        self.col_dim = 1\n",
        "\n",
        "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
        "   \n",
        "        q = self.W_q(encodings_for_q)\n",
        "        k = self.W_k(encodings_for_k)\n",
        "        v = self.W_v(encodings_for_v)\n",
        "\n",
        "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
        "        scaled_sims = sims / torch.tensor(q.size(self.col_dim)**0.5)\n",
        "\n",
        "        if mask is not None:\n",
        " \n",
        "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9) # I've also seen -1e20 and -9e15 used in masking\n",
        "\n",
        "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
        "\n",
        "        attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "        return attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d32eab2-e3e3-4979-a0b2-749815841554",
      "metadata": {
        "id": "2d32eab2-e3e3-4979-a0b2-749815841554"
      },
      "source": [
        "<a id=\"encoder\"></a>\n",
        "# The Encoder Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "737def7d-e0c7-4b73-b447-5bbcd798103f",
      "metadata": {
        "id": "737def7d-e0c7-4b73-b447-5bbcd798103f"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_tokens=4, d_model=2, max_len=3):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        L.seed_everything(seed=42)\n",
        "\n",
        "        self.we = nn.Embedding(num_embeddings=num_tokens,\n",
        "                               embedding_dim=d_model)\n",
        "\n",
        "        self.pe = PositionEncoding(d_model=d_model,\n",
        "                                   max_len=max_len)\n",
        "\n",
        "        self.self_attention = Attention(d_model=d_model)\n",
        "\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "\n",
        "        word_embeddings = self.we(token_ids)\n",
        "\n",
        "        position_encoded = self.pe(word_embeddings)\n",
        "\n",
        "        self_attention_values = self.self_attention(position_encoded,\n",
        "                                                    position_encoded,\n",
        "                                                    position_encoded)\n",
        "\n",
        "        output_values = position_encoded + self_attention_values\n",
        "\n",
        "        return output_values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791ffd7e-6671-4153-8d47-4c3b74e61341",
      "metadata": {
        "id": "791ffd7e-6671-4153-8d47-4c3b74e61341"
      },
      "source": [
        "<a id=\"decoder\"></a>\n",
        "# The Decoder Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1f286589-e933-46d2-aeda-600c74799357",
      "metadata": {
        "id": "1f286589-e933-46d2-aeda-600c74799357"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_tokens=4, d_model=2, max_len=3):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        L.seed_everything(seed=43)\n",
        "\n",
        "        self.we = nn.Embedding(num_embeddings=num_tokens,\n",
        "                               embedding_dim=d_model)\n",
        "\n",
        "        self.pe = PositionEncoding(d_model=d_model,\n",
        "                                   max_len=max_len)\n",
        "\n",
        "        self.self_attention = Attention(d_model=d_model)\n",
        "\n",
        "        self.enc_dec_attention = Attention(d_model=d_model)\n",
        "\n",
        "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
        "\n",
        "        self.row_dim = 0\n",
        "        self.col_dim = 1\n",
        "\n",
        "\n",
        "    def forward(self, token_ids, encoder_values):\n",
        "     word_embeddings = self.we(token_ids)\n",
        "     position_encoded = self.pe(word_embeddings)\n",
        "\n",
        "     # Create mask and move it to the same device as `position_encoded`\n",
        "     mask = torch.tril(torch.ones((token_ids.size(self.row_dim), token_ids.size(self.row_dim)))).to(position_encoded.device)\n",
        "     mask = mask == 0  # Invert the mask for masked_fill\n",
        "\n",
        "     self_attention_values = self.self_attention(position_encoded,\n",
        "                                                position_encoded,\n",
        "                                                position_encoded,\n",
        "                                                mask=mask)\n",
        "\n",
        "     residual_connection_values = position_encoded + self_attention_values\n",
        "\n",
        "     enc_dec_attention_values = self.enc_dec_attention(residual_connection_values,\n",
        "                                                      encoder_values,\n",
        "                                                      encoder_values)\n",
        "\n",
        "     residual_connection_values = enc_dec_attention_values + residual_connection_values\n",
        "\n",
        "     fc_layer_output = self.fc_layer(residual_connection_values)\n",
        "\n",
        "     return fc_layer_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "146c5303-6195-4bc7-8ff5-178e8ed2b58f",
      "metadata": {
        "id": "146c5303-6195-4bc7-8ff5-178e8ed2b58f"
      },
      "source": [
        "<a id=\"transformer\"></a>\n",
        "# The Transformer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a82e4dd3-a141-4df9-aaae-53f561e6defe",
      "metadata": {
        "id": "a82e4dd3-a141-4df9-aaae-53f561e6defe"
      },
      "outputs": [],
      "source": [
        "class Transformer(L.LightningModule):\n",
        "\n",
        "    def __init__(self, input_size, output_size, d_model=2, max_len=3):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_tokens=len(input_vocab), d_model=d_model, max_len=max_len)\n",
        "        self.decoder = Decoder(num_tokens=len(output_vocab), d_model=d_model, max_len=max_len)\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def forward(self, inputs, labels):\n",
        "\n",
        "        encoder_values = self.encoder(inputs)\n",
        "        output_presoftmax = self.decoder(labels, encoder_values)\n",
        "\n",
        "        return(output_presoftmax)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        return Adam(self.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      input_i, label_i = batch  # collect input\n",
        "\n",
        "      # Ensure tensors are on the same device\n",
        "      input_tokens = torch.cat((torch.tensor([0], device=input_i.device), input_i[0]))\n",
        "      teacher_forcing = torch.cat((torch.tensor([0], device=label_i.device), label_i[0]))\n",
        "      expected_output = torch.cat((label_i[0], torch.tensor([4], device=label_i.device)))\n",
        "\n",
        "      # Forward pass\n",
        "      output_i = self.forward(input_tokens, teacher_forcing)\n",
        "      loss = self.loss(output_i, expected_output)\n",
        "\n",
        "      return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4d882fcd-77ec-4279-8e10-bc16fa067ecb",
      "metadata": {
        "id": "4d882fcd-77ec-4279-8e10-bc16fa067ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "predicted_ids: tensor([0, 2, 0, 1])\n"
          ]
        }
      ],
      "source": [
        "## First, a reminder of our input and output vocabularies...\n",
        "# input_vocab = {'<SOS>': 0, # Start\n",
        "#                'lets': 1,\n",
        "#                'to': 2,\n",
        "#                'go': 3}\n",
        "\n",
        "# output_vocab = {'<SOS>': 0, # Start\n",
        "#                 'ir': 1,\n",
        "#                 'vamos': 2,\n",
        "#                 'y': 3,\n",
        "#                 '<EOS>': 4} # End\n",
        "max_length = 3\n",
        "\n",
        "## Create a tranformer object...\n",
        "transformer = Transformer(len(input_vocab), len(output_vocab), d_model=2, max_len=max_length)\n",
        "\n",
        "## Encode the user input...\n",
        "encoder_values = transformer.encoder(torch.tensor([0, 1, 3])) \n",
        "\n",
        "# Initialize predicted_ids with <SOS> token\n",
        "predicted_ids = torch.tensor([0]) \n",
        "\n",
        "for i in range(max_length):\n",
        "\n",
        "    prediction = transformer.decoder(predicted_ids, encoder_values)\n",
        "\n",
        "    predicted_id = torch.tensor([torch.argmax(prediction[-1,:])])\n",
        "    ## add the predicted token id to the list of predicted ids.\n",
        "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "    if (predicted_id == 4): ## if the prediction is <EOS> then we are done.\n",
        "        break\n",
        "\n",
        "print(\"\\npredicted_ids:\", predicted_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "772b08fa",
      "metadata": {},
      "source": [
        "And, without training, the transformer predicts **\\<SOS> vamos \\<SOS> ir**, but we wanted it to predict **\\<SOS> vamos \\<EOS>** So, since the transformer didn't correctly translate the English phrases into Spanish, we'll have to train it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982ca23f-0c46-4aa9-aebf-32e5bcedece6",
      "metadata": {
        "id": "982ca23f-0c46-4aa9-aebf-32e5bcedece6"
      },
      "source": [
        "<a id=\"train\"></a>\n",
        "# Train the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "16e3404f-af9d-414a-9886-2c7138a8cc21",
      "metadata": {
        "id": "16e3404f-af9d-414a-9886-2c7138a8cc21"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(len(input_vocab), len(output_vocab), d_model=2, max_len=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a96bd93c-31f4-4ef8-8c4f-fd33c3c20344",
      "metadata": {
        "id": "a96bd93c-31f4-4ef8-8c4f-fd33c3c20344",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a4e34f2b14e4173afb5ed530dfbe18d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = L.Trainer(max_epochs=30)\n",
        "trainer.fit(transformer, train_dataloaders=dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c040285c-c6de-4aac-b5b9-925e84f5e83f",
      "metadata": {
        "id": "c040285c-c6de-4aac-b5b9-925e84f5e83f"
      },
      "source": [
        "<a id=\"use\"></a>\n",
        "# Use the Trained Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "717eaa4e-2249-46a1-b01f-9ed76beed0de",
      "metadata": {
        "id": "717eaa4e-2249-46a1-b01f-9ed76beed0de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "predicted_ids: tensor([0, 2, 4])\n"
          ]
        }
      ],
      "source": [
        "## First, a reminder of our input and output vocabularies...\n",
        "# input_vocab = {'<SOS>': 0, # Start\n",
        "#                'lets': 1,\n",
        "#                'to': 2,\n",
        "#                'go': 3}\n",
        "\n",
        "# output_vocab = {'<SOS>': 0, # Start\n",
        "#                 'ir': 1,\n",
        "#                 'vamos': 2,\n",
        "#                 'y': 3,\n",
        "#                 '<EOS>': 4} # End\n",
        "\n",
        "max_length = 3\n",
        "row_dim = 0\n",
        "col_dim = 1\n",
        "\n",
        "encoder_values = transformer.encoder(torch.tensor([0, 1, 3])) \n",
        "\n",
        "predicted_ids = torch.tensor([0]) \n",
        "\n",
        "for i in range(max_length):\n",
        "\n",
        "    prediction = transformer.decoder(predicted_ids, encoder_values)\n",
        "\n",
        "    predicted_id = torch.tensor([torch.argmax(prediction[-1,:])])\n",
        "    \n",
        "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "    if (predicted_id == 4): \n",
        "        break\n",
        "\n",
        "print(\"\\npredicted_ids:\", predicted_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "127ebe12",
      "metadata": {},
      "source": [
        "And the output is **\\<SOS> vamos \\<EOS>**, which is exactly what we want."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
